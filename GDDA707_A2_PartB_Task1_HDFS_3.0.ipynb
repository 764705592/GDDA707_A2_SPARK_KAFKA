{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5eae2cd-d221-4b91-8124-c53b33fa238e",
   "metadata": {},
   "source": [
    "### Part B. Task 1. - Data Ingestion Pipeline ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221cf2bb-264b-44d2-bd3c-191c4749d2ca",
   "metadata": {},
   "source": [
    "#### Docker Desktop Environment ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55a02492-c78d-4182-998e-c5a489bd0bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >>>>>>>> Data Ingestion Pipeline >>>>>>>> .\n",
      "SUCCESSFUL: Data Validation.\n",
      "HDFS Directory: /user/data/bitcoin_transactions\n",
      "Copy File to Docker Container: Completed: /tmp/GDDA707_A2_Processed_Bitcoin_Transaction_HDFS.csv\n",
      "SUCCESSFULLY UPLOADED C:/hadoopsetup/data/GDDA707_A2_Processed_Bitcoin_Transaction_HDFS.csv to HDFS at /user/data/bitcoin_transactions\n",
      "DATA INGESTION PIPELINE COMPLETED!!!.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# CONFIGURATIONS\n",
    "LOCAL_FILE_PATH = 'C:/hadoopsetup/data/GDDA707_A2_Processed_Bitcoin_Transaction_HDFS.csv'\n",
    "HDFS_DIRECTORY = '/user/data/bitcoin_transactions'\n",
    "LOG_FILE = 'bitcoin_ingestion_log.txt'\n",
    "HADOOP_CONTAINER_NAME = \"namenode\"\n",
    "\n",
    "def log_message(message):\n",
    "    \"\"\"Logs messages to a log file for tracking the ingestion process.\"\"\"\n",
    "    with open(LOG_FILE, 'a') as log:\n",
    "        log.write(f\"{datetime.now()}: {message}\\n\")\n",
    "    print(message)\n",
    "\n",
    "def validate_data(file_path):\n",
    "    \"\"\"\n",
    "    Validates the data to ensure integrity before loading into HDFS.\n",
    "    - Checks for missing values\n",
    "    - Validates date format\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "\n",
    "        # Check for missing values\n",
    "        if data.isnull().values.any():\n",
    "            raise ValueError(\"Data contains missing values. Please clean the data.\")\n",
    "\n",
    "        # Validate 'Date' column format (ISO 8601 format)\n",
    "        pd.to_datetime(data['Date'], format='%Y-%m-%d')  \n",
    "\n",
    "        log_message(\"SUCCESSFUL: Data Validation.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log_message(f\"FAILED: Data Validation! : {e}\")\n",
    "        return False\n",
    "\n",
    "def upload_to_hdfs(local_path, hdfs_path, container_name):\n",
    "    \"\"\"\n",
    "    Uploads the file to HDFS using the Docker namenode container.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # HDFS directory confirmation\n",
    "        mkdir_command = f\"hadoop fs -mkdir -p {hdfs_path}\"\n",
    "        subprocess.run(\n",
    "            [\"docker\", \"exec\", container_name, \"bash\", \"-c\", mkdir_command],\n",
    "            check=True,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "        )\n",
    "        log_message(f\"HDFS Directory: {hdfs_path}\")\n",
    "\n",
    "        # Copy to Docker container\n",
    "        container_tmp_path = f\"/tmp/{os.path.basename(local_path)}\"\n",
    "        subprocess.run(\n",
    "            [\"docker\", \"cp\", local_path, f\"{container_name}:{container_tmp_path}\"],\n",
    "            check=True,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "        )\n",
    "        log_message(f\"Copy File to Docker Container: Completed: {container_tmp_path}\")\n",
    "\n",
    "        # Upload the file to HDFS from the Docker container\n",
    "        put_command = f\"hadoop fs -put -f {container_tmp_path} {hdfs_path}\"\n",
    "        subprocess.run(\n",
    "            [\"docker\", \"exec\", container_name, \"bash\", \"-c\", put_command],\n",
    "            check=True,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "        )\n",
    "        log_message(f\"SUCCESSFULLY UPLOADED {local_path} to HDFS at {hdfs_path}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        log_message(f\"FAILED TO UPLOAD FILE: {e.stderr.strip()}\")\n",
    "\n",
    "def main_pipeline():\n",
    "    \"\"\"Main function to execute the data ingestion pipeline.\"\"\"\n",
    "    log_message(\" >>>>>>>> Data Ingestion Pipeline >>>>>>>> .\")\n",
    "\n",
    "    # Step 1: Validate Data\n",
    "    if validate_data(LOCAL_FILE_PATH):\n",
    "        # Step 2: Upload to HDFS\n",
    "        upload_to_hdfs(LOCAL_FILE_PATH, HDFS_DIRECTORY, HADOOP_CONTAINER_NAME)\n",
    "\n",
    "    log_message(\"DATA INGESTION PIPELINE COMPLETED!!!.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9eaad-3aaa-468c-bc8d-b84dd267c163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
